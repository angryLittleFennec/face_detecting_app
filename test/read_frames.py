import logging
import os
import cv2
import numpy as np
import dlib
import requests
from ultralytics import YOLO
from threading import Thread, Lock
from queue import Queue
import time
from datetime import datetime
import torch
import torch.cuda
from torch.cuda import Stream

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ RTSP
RTSP_INPUT_URL = os.getenv("RTSP_IN", "rtsp://mediamtx-svc:8554/mediamtx/stream3")
RTSP_OUTPUT_URL = os.getenv("RTSP_OUT", "rtsp://mediamtx-svc:8554/mediamtx/newstream1")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('frame_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
if DEVICE == 'cuda':
    torch.backends.cudnn.benchmark = True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    torch.backends.cudnn.deterministic = False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    torch.cuda.empty_cache()  # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
    CUDA_STREAM = Stream()  # –°–æ–∑–¥–∞–µ–º CUDA –ø–æ—Ç–æ–∫ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

logger.info(f"Using device: {DEVICE}")
if DEVICE == 'cuda':
    logger.info(f"CUDA Device: {torch.cuda.get_device_name(0)}")
    logger.info(f"CUDA Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
    logger.info(f"CUDA Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB")

logger.info(f"RTSP_INPUT_URL: {RTSP_INPUT_URL}")
logger.info(f"RTSP_OUTPUT_URL: {RTSP_OUTPUT_URL}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
face_lock = Lock()
used_faces = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ª–∏—Ü
url = "http://face-recognition-svc:80/find_face"
LOGGING_SERVICE_URL = "http://logging_service:8000/log"

# –û—á–µ—Ä–µ–¥—å –¥–ª—è –∫–∞–¥—Ä–æ–≤
frame_queue = Queue(maxsize=30)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
FACE_MODEL = YOLO("ml_models/yolov8n-face.pt")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è YOLO
FACE_MODEL.conf = 0.7  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
FACE_MODEL.iou = 0.3   # –ù–∏–∑–∫–∏–π IoU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
FACE_MODEL.verbose = False
FACE_MODEL.max_det = 3  # –£–º–µ–Ω—å—à–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–π
FACE_MODEL.agnostic = True
FACE_MODEL.classes = [0]  # –¢–æ–ª—å–∫–æ –ª–∏—Ü–∞

# –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
FACE_MODEL.to(DEVICE)
logger.info(f"YOLO model moved to {DEVICE}")

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ª–∏—Ü
face_detector = dlib.get_frontal_face_detector()
shape_predictor = dlib.shape_predictor("ml_models/shape_predictor_68_face_landmarks.dat")
face_rec_model = dlib.face_recognition_model_v1("ml_models/dlib_face_recognition_resnet_model_v1.dat")

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª–∏—Ü–∞—Ö
tracked_faces = {}  # {track_id: {"name": name, "first_seen": timestamp, "last_seen": timestamp}}
active_tracks = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö track_id –≤ —Ç–µ–∫—É—â–µ–º –∫–∞–¥—Ä–µ
frame_count = 0

def log_face_event(track_id, event_type, name=None):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π —Å –ª–∏—Ü–∞–º–∏"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if event_type == "enter":
        logger.info(f"üü¢ {timestamp} - –ß–µ–ª–æ–≤–µ–∫ {name or track_id} –≤–æ—à–µ–ª –≤ –∫–∞–¥—Ä")
    elif event_type == "exit":
        duration = tracked_faces[track_id]["last_seen"] - tracked_faces[track_id]["first_seen"]
        logger.info(f"üî¥ {timestamp} - –ß–µ–ª–æ–≤–µ–∫ {name or track_id} –≤—ã—à–µ–ª –∏–∑ –∫–∞–¥—Ä–∞. –í—Ä–µ–º—è –≤ –∫–∞–¥—Ä–µ: {duration:.1f} —Å–µ–∫")
    elif event_type == "recognized":
        logger.info(f"üë§ {timestamp} - –†–∞—Å–ø–æ–∑–Ω–∞–Ω —á–µ–ª–æ–≤–µ–∫: {name} (ID: {track_id})")

def get_face_embedding(image: np.ndarray):
    if image.size == 0:
        return None

    try:
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        dets = face_detector(rgb_image, 1)
        if len(dets) == 0:
            return None

        shape = shape_predictor(rgb_image, dets[0])
        face_descriptor = face_rec_model.compute_face_descriptor(rgb_image, shape, num_jitters=1)
        return np.array(face_descriptor)
    except Exception as e:
        logger.error(f"Error in get_face_embedding: {e}")
        return None

def find_matching_face(embedding):
    try:
        encoding_str = ",".join(map(str, embedding))
        params = {"embedding_str": encoding_str}
        response = requests.get(url, params=params, timeout=1)
        if response.status_code == 200:
            js = response.json()
            return js['person'] if js['status'] == 'success' else None
    except Exception as e:
        logger.error(f"Error in find_matching_face: {e}")
    return None

def process_frame(frame, frame_id, out):
    global frame_count, active_tracks
    start_time = time.time()
    try:
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–¥—Ä–∞
        copy_start = time.time()
        annotated_frame = frame.copy()
        copy_time = time.time() - copy_start
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        resize_start = time.time()
        scale_percent = 20  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        width = int(frame.shape[1] * scale_percent / 100)
        height = int(frame.shape[0] * scale_percent / 100)
        resized_frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_LINEAR)
        resize_time = time.time() - resize_start
        
        logger.info(f"Original frame size: {frame.shape}, Resized frame size: {resized_frame.shape}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º YOLO —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –Ω–∞ GPU —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        yolo_start = time.time()
        with torch.cuda.amp.autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–º–µ—à–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
            with torch.no_grad():
                if DEVICE == 'cuda':
                    with torch.cuda.stream(CUDA_STREAM):
                        results = FACE_MODEL.track(
                            resized_frame,
                            verbose=False,
                            stream=False,
                            persist=True,
                            conf=0.7,
                            iou=0.3,
                            max_det=3,
                            device=DEVICE
                        )
                        torch.cuda.current_stream().synchronize()  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º CUDA –ø–æ—Ç–æ–∫
                else:
                    results = FACE_MODEL.track(
                        resized_frame,
                        verbose=False,
                        stream=False,
                        persist=True,
                        conf=0.7,
                        iou=0.3,
                        max_det=3,
                        device=DEVICE
                    )
        yolo_time = time.time() - yolo_start

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
        if DEVICE == 'cuda':
            torch.cuda.empty_cache()  # –û—á–∏—â–∞–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –ø–∞–º—è—Ç—å GPU

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        scale_x = frame.shape[1] / width
        scale_y = frame.shape[0] / height
        
        face_recognition_time = 0
        faces_processed = 0
        
        # –û—á–∏—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–∞–¥—Ä–∞
        current_tracks = set()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        process_results_start = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –±–æ–∫—Å—ã —Å—Ä–∞–∑—É —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
        boxes_start = time.time()
        boxes = []
        if results[0].boxes.id is not None:
            if DEVICE == 'cuda':
                boxes = results[0].boxes.cpu()  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ CPU –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            else:
                boxes = results[0].boxes
        boxes_time = time.time() - boxes_start
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        scaling_start = time.time()
        scaled_boxes = []
        if len(boxes) > 0:
            for box in boxes:
                if int(box.cls) == 0:  # –ö–ª–∞—Å—Å 0 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏—Ü—É
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    track_id = int(box.id.item())
                    
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    x1 = int(x1 * scale_x)
                    y1 = int(y1 * scale_y)
                    x2 = int(x2 * scale_x)
                    y2 = int(y2 * scale_y)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—Ç—É–ø—ã
                    padding = 20
                    x1 = max(0, x1 - padding)
                    y1 = max(0, y1 - padding)
                    x2 = min(frame.shape[1], x2 + padding)
                    y2 = min(frame.shape[0], y2 + padding)
                    
                    scaled_boxes.append({
                        'track_id': track_id,
                        'coords': (x1, y1, x2, y2),
                        'face_image': frame[y1:y2, x1:x2]
                    })
        scaling_time = time.time() - scaling_start
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –±–æ–∫—Å—ã
        processing_start = time.time()
        face_recognition_total = 0
        face_embedding_total = 0
        face_matching_total = 0
        drawing_total = 0
        
        for box_data in scaled_boxes:
            faces_processed += 1
            track_id = box_data['track_id']
            x1, y1, x2, y2 = box_data['coords']
            face_image = box_data['face_image']
            current_tracks.add(track_id)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–æ–≤—ã–π –ª–∏ —ç—Ç–æ —Ç—Ä–µ–∫
            if track_id not in tracked_faces:
                tracked_faces[track_id] = {
                    "name": None,
                    "first_seen": time.time(),
                    "last_seen": time.time()
                }
                log_face_event(track_id, "enter")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è
            tracked_faces[track_id]["last_seen"] = time.time()
            
            # –ï—Å–ª–∏ –ª–∏—Ü–æ –µ—â–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ, –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å
            if tracked_faces[track_id]["name"] is None:
                face_start = time.time()
                embedding = get_face_embedding(face_image)
                face_embedding_total += time.time() - face_start
                
                if embedding is not None:
                    match_start = time.time()
                    # with face_lock:
                    #     match = find_matching_face(embedding)
                    #     if match:
                    #         tracked_faces[track_id]["name"] = match
                    #         log_face_event(track_id, "recognized", match)
                    face_matching_total += time.time() - match_start
                face_recognition_total += time.time() - face_start
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–∏—Ü–µ
            draw_start = time.time()
            name = tracked_faces[track_id]["name"] or f"Unknown-{track_id}"
            cv2.putText(annotated_frame, name, (x1 + 100, y1 - 10),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
            cv2.putText(annotated_frame, f"ID: {track_id}", (x1, y1 - 10),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
            drawing_total += time.time() - draw_start
        
        processing_time = time.time() - processing_start
        process_results_time = time.time() - process_results_start
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Ç—Ä–µ–∫–∏ –∏—Å—á–µ–∑–ª–∏
        check_tracks_start = time.time()
        for track_id in active_tracks - current_tracks:
            if track_id in tracked_faces:
                log_face_event(track_id, "exit", tracked_faces[track_id]["name"])
        check_tracks_time = time.time() - check_tracks_start
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
        active_tracks = current_tracks
        
        total_time = time.time() - start_time
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π
        logger.info(
            f"Frame {frame_id} processed in {process_results_time:.3f}s | "
            f"Copy: {copy_time:.3f}s | "
            f"Resize: {resize_time:.3f}s | "
            f"YOLO: {yolo_time:.3f}s | "
            f"Process results: {process_results_time:.3f}s ("
            f"boxes: {boxes_time:.3f}s, "
            f"scaling: {scaling_time:.3f}s, "
            f"processing: {processing_time:.3f}s ["
            f"embedding: {face_embedding_total:.3f}s, "
            f"matching: {face_matching_total:.3f}s, "
            f"drawing: {drawing_total:.3f}s]) | "
            f"Faces detected: {faces_processed} | "
            f"Active tracks: {len(active_tracks)}"
        )
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä
        out.write(annotated_frame)
        return True
    except Exception as e:
        logger.error(f"Error in process_frame: {e}")
        return False

def process_frames(out):
    frame_count = 0
    total_processing_time = 0
    
    while True:
        if not frame_queue.empty():
            frame = frame_queue.get()
            if frame is None:
                if frame_count > 0:
                    avg_time = total_processing_time / frame_count
                    logger.info(f"Average frame processing time: {avg_time:.3f}s over {frame_count} frames")
                break
            
            start_time = time.time()
            success = process_frame(frame, frame_count, out)
            processing_time = time.time() - start_time
            
            if success:
                total_processing_time += processing_time
                frame_count += 1

def open_capture_with_retry(url, max_retries=5, retry_delay=3):
    for attempt in range(max_retries):
        cap = cv2.VideoCapture(url)
        if cap.isOpened():
            print(f"‚úÖ Connected to stream on attempt {attempt+1}")
            return cap
        print(f"‚ö†Ô∏è Connection attempt {attempt+1}/{max_retries} failed, retrying in {retry_delay}s...")
        time.sleep(retry_delay)
    return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞—Ö–≤–∞—Ç–∞ –≤–∏–¥–µ–æ
cap = open_capture_with_retry(RTSP_INPUT_URL)
if cap is None:
    print("‚ùå Error: Could not connect to RTSP stream after multiple attempts!")
    exit()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∏–¥–µ–æ
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS) or 30

print(f"üé• Input stream opened: {frame_width}x{frame_height} at {fps} FPS")
print(f"üîÑ Forwarding to {RTSP_OUTPUT_URL}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GStreamer –¥–ª—è –≤—ã–≤–æ–¥–∞ RTSP —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
out = cv2.VideoWriter(
    f'appsrc ! videoconvert ! video/x-raw,format=I420 ! '
    f'x264enc speed-preset=ultrafast bitrate=1024 key-int-max={int(fps*2)} ! '
    f'video/x-h264,profile=baseline ! rtspclientsink protocols=tcp location={RTSP_OUTPUT_URL}',
    cv2.CAP_GSTREAMER, 0, fps, (frame_width, frame_height), True
)

if not out.isOpened():
    print("‚ùå Error: Cannot open RTSP output stream!")
    cap.release()
    exit()

# –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
processor_thread = Thread(target=process_frames, args=(out,))
processor_thread.start()

# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
frame_count = 0
start_time = time.time()

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("‚ùå Error: Failed to read frame from RTSP stream!")
        break

    frame_queue.put(frame)
    frame_count += 1

    # –í—ã–≤–æ–¥ FPS –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
    if frame_count % 900 == 0:  # 30 —Å–µ–∫—É–Ω–¥ –ø—Ä–∏ 30 FPS
        elapsed_time = time.time() - start_time
        current_fps = frame_count / elapsed_time
        print(f"Current FPS: {current_fps:.2f}")
        frame_count = 0
        start_time = time.time()

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞
frame_queue.put(None)
processor_thread.join()

# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
out.release()
cap.release()