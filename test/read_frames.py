import logging
import os
import cv2
import numpy as np
import dlib
import requests
from ultralytics import YOLO
from threading import Thread, Lock
from queue import Queue
import time
from datetime import datetime
import torch
import torch.cuda
from torch.cuda import Stream
from sqlalchemy import Column, ForeignKey, Integer, String, Text, DateTime, create_engine
from sqlalchemy.orm import sessionmaker, Session, relationship
from sqlalchemy.ext.declarative import declarative_base
from typing import Dict, Tuple, List

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ RTSP
RTSP_INPUT_URL = os.getenv("RTSP_IN", "rtsp://mediamtx-svc:8554/mediamtx/stream3")
RTSP_OUTPUT_URL = os.getenv("RTSP_OUT", "rtsp://mediamtx-svc:8554/mediamtx/newstream1")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('frame_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
if DEVICE == 'cuda':
    torch.backends.cudnn.benchmark = True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    torch.backends.cudnn.deterministic = False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    torch.cuda.empty_cache()  # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
    CUDA_STREAM = Stream()  # –°–æ–∑–¥–∞–µ–º CUDA –ø–æ—Ç–æ–∫ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

logger.info(f"Using device: {DEVICE}")
if DEVICE == 'cuda':
    logger.info(f"CUDA Device: {torch.cuda.get_device_name(0)}")
    logger.info(f"CUDA Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
    logger.info(f"CUDA Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB")

logger.info(f"RTSP_INPUT_URL: {RTSP_INPUT_URL}")
logger.info(f"RTSP_OUTPUT_URL: {RTSP_OUTPUT_URL}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
face_lock = Lock()
used_faces = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ª–∏—Ü
url = "http://face-recognition-svc:80/find_face"
LOGGING_SERVICE_URL = "http://logging_service:8000/log"

# –û—á–µ—Ä–µ–¥—å –¥–ª—è –∫–∞–¥—Ä–æ–≤
frame_queue = Queue(maxsize=30)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
FACE_MODEL = YOLO("ml_models/yolov8n-face.pt")

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è YOLO
FACE_MODEL.conf = 0.7  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
FACE_MODEL.iou = 0.3   # –ù–∏–∑–∫–∏–π IoU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
FACE_MODEL.verbose = False
FACE_MODEL.max_det = 3  # –£–º–µ–Ω—å—à–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–π
FACE_MODEL.agnostic = True
FACE_MODEL.classes = [0]  # –¢–æ–ª—å–∫–æ –ª–∏—Ü–∞

# –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
FACE_MODEL.to(DEVICE)
logger.info(f"YOLO model moved to {DEVICE}")

# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ª–∏—Ü
face_detector = dlib.get_frontal_face_detector()
shape_predictor = dlib.shape_predictor("ml_models/shape_predictor_68_face_landmarks.dat")
face_rec_model = dlib.face_recognition_model_v1("ml_models/dlib_face_recognition_resnet_model_v1.dat")

# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª–∏—Ü–∞—Ö
tracked_faces = {}  # {track_id: {"name": name, "first_seen": timestamp, "last_seen": timestamp}}
active_tracks = set()  # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö track_id –≤ —Ç–µ–∫—É—â–µ–º –∫–∞–¥—Ä–µ
frame_count = 0

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://admin:secret@postgres-postgresql:5432/surveillance_db")
Base = declarative_base()

# –ú–æ–¥–µ–ª–∏ SQLAlchemy
class Person(Base):
    __tablename__ = "persons"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, nullable=False)
    faces = relationship("Face", back_populates="person", cascade="all, delete-orphan")

class Face(Base):
    __tablename__ = "faces"
    id = Column(Integer, primary_key=True, index=True)
    person_id = Column(Integer, ForeignKey('persons.id', ondelete="CASCADE"))
    encoding = Column(Text, nullable=False)
    person = relationship("Person", back_populates="faces")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(bind=engine)

# –ö—ç—à –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ª–∏—Ü
face_embeddings_cache: Dict[int, Tuple[np.ndarray, str]] = {}

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def load_face_embeddings():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ª–∏—Ü –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à"""
    global face_embeddings_cache
    try:
        db = SessionLocal()
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ª–∏—Ü–∞ —Å –∏–º–µ–Ω–∞–º–∏ –ª—é–¥–µ–π
        faces = db.query(Face, Person.name).join(Person).all()
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à
        face_embeddings_cache.clear()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ –∫—ç—à
        for face, person_name in faces:
            try:
                encoding = np.fromstring(face.encoding, sep=',')
                if encoding.shape == (128,):
                    face_embeddings_cache[face.id] = (encoding, person_name)
            except Exception as e:
                logger.error(f"Error loading face {face.id}: {e}")
                continue
                
        logger.info(f"Loaded {len(face_embeddings_cache)} face embeddings into cache")
    except Exception as e:
        logger.error(f"Error loading face embeddings: {e}")
    finally:
        db.close()

# –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
load_face_embeddings()

def find_matching_face(embedding: np.ndarray) -> str:
    """–ò—â–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ª–∏—Ü –≤ –∫—ç—à–µ"""
    try:
        if not isinstance(embedding, np.ndarray):
            logger.error("Error: embedding is not a numpy array")
            return None
            
        if embedding.shape != (128,):
            logger.error(f"Error: invalid embedding shape: {embedding.shape}")
            return None
            
        min_dist = float("inf")
        best_match = None
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –∫—ç—à–µ
        for face_id, (cached_encoding, person_name) in face_embeddings_cache.items():
            try:
                # –í—ã—á–∏—Å–ª—è–µ–º –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                dist = np.linalg.norm(cached_encoding - embedding)
                
                # –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ, –æ–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if dist < 0.6 and dist < min_dist:
                    min_dist = dist
                    best_match = person_name
                    
            except Exception as e:
                logger.error(f"Error processing face {face_id}: {e}")
                continue
        
        if best_match:
            logger.info(f"Found matching person: {best_match}")
            return best_match
        else:
            logger.info("No matching face found")
            return None
            
    except Exception as e:
        logger.error(f"Error in find_matching_face: {e}")
        return None

def log_face_event(track_id, event_type, name=None):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π —Å –ª–∏—Ü–∞–º–∏"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º ID —á–µ–ª–æ–≤–µ–∫–∞ –ø–æ –∏–º–µ–Ω–∏
        person_id = None
        if name:
            db = SessionLocal()
            person = db.query(Person).filter(Person.name == name).first()
            if person:
                person_id = person.id
            db.close()
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π
        if event_type in ["enter", "exit"]:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤—Ö–æ–¥—ã –∏ –≤—ã—Ö–æ–¥—ã
            db = SessionLocal()
            event_service = EventService(db)
            event_service.create_event(
                event_type=EventType(event_type),
                person_id=person_id,
                stream_processor_id=1,  # TODO: –ü–æ–ª—É—á–∞—Ç—å ID –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                track_id=track_id,
                duration=tracked_faces[track_id]["last_seen"] - tracked_faces[track_id]["first_seen"] if event_type == "exit" else None
            )
            db.close()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if event_type == "enter":
            if name:
                logger.info(f"üü¢ {timestamp} - {name} –≤–æ—à–µ–ª –≤ –∫–∞–¥—Ä")
            else:
                logger.info(f"üü¢ {timestamp} - –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ (ID: {track_id}) –≤–æ—à–µ–ª –≤ –∫–∞–¥—Ä")
        elif event_type == "recognized":
            logger.info(f"üë§ {timestamp} - –†–∞—Å–ø–æ–∑–Ω–∞–Ω —á–µ–ª–æ–≤–µ–∫: {name} (ID: {track_id})")
        elif event_type == "exit":
            if tracked_faces[track_id]["name"]:
                duration = tracked_faces[track_id]["last_seen"] - tracked_faces[track_id]["first_seen"]
                logger.info(f"üî¥ {timestamp} - {tracked_faces[track_id]['name']} –≤—ã—à–µ–ª –∏–∑ –∫–∞–¥—Ä–∞. –í—Ä–µ–º—è –≤ –∫–∞–¥—Ä–µ: {duration:.1f} —Å–µ–∫")
            else:
                duration = tracked_faces[track_id]["last_seen"] - tracked_faces[track_id]["first_seen"]
                logger.info(f"üî¥ {timestamp} - –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —á–µ–ª–æ–≤–µ–∫ (ID: {track_id}) –≤—ã—à–µ–ª –∏–∑ –∫–∞–¥—Ä–∞. –í—Ä–µ–º—è –≤ –∫–∞–¥—Ä–µ: {duration:.1f} —Å–µ–∫")
    except Exception as e:
        logger.error(f"Error logging face event: {e}")

def get_face_embedding(image: np.ndarray):
    if image.size == 0:
        return None

    try:
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        dets = face_detector(rgb_image, 1)
        if len(dets) == 0:
            return None

        shape = shape_predictor(rgb_image, dets[0])
        face_descriptor = face_rec_model.compute_face_descriptor(rgb_image, shape, num_jitters=1)
        return np.array(face_descriptor)
    except Exception as e:
        logger.error(f"Error in get_face_embedding: {e}")
        return None

def process_frame(frame, frame_id, out):
    global frame_count, active_tracks
    start_time = time.time()
    try:
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–¥—Ä–∞
        copy_start = time.time()
        annotated_frame = frame.copy()
        copy_time = time.time() - copy_start
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        resize_start = time.time()
        scale_percent = 20  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        width = int(frame.shape[1] * scale_percent / 100)
        height = int(frame.shape[0] * scale_percent / 100)
        resized_frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_LINEAR)
        resize_time = time.time() - resize_start
        
        logger.info(f"Original frame size: {frame.shape}, Resized frame size: {resized_frame.shape}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º YOLO —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –Ω–∞ GPU —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        yolo_start = time.time()
        with torch.cuda.amp.autocast():  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–º–µ—à–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
            with torch.no_grad():
                if DEVICE == 'cuda':
                    with torch.cuda.stream(CUDA_STREAM):
                        results = FACE_MODEL.track(
                            resized_frame,
                            verbose=False,
                            stream=False,
                            persist=True,
                            conf=0.7,
                            iou=0.3,
                            max_det=3,
                            device=DEVICE
                        )
                        torch.cuda.current_stream().synchronize()  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º CUDA –ø–æ—Ç–æ–∫
                else:
                    results = FACE_MODEL.track(
                        resized_frame,
                        verbose=False,
                        stream=False,
                        persist=True,
                        conf=0.7,
                        iou=0.3,
                        max_det=3,
                        device=DEVICE
                    )
        yolo_time = time.time() - yolo_start

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
        if DEVICE == 'cuda':
            torch.cuda.empty_cache()  # –û—á–∏—â–∞–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –ø–∞–º—è—Ç—å GPU

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—Ä–∞—Ç–Ω–æ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        scale_x = frame.shape[1] / width
        scale_y = frame.shape[0] / height
        
        face_recognition_time = 0
        faces_processed = 0
        
        # –û—á–∏—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–∞–¥—Ä–∞
        current_tracks = set()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        process_results_start = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –±–æ–∫—Å—ã —Å—Ä–∞–∑—É —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è GPU
        boxes_start = time.time()
        boxes = []
        if results[0].boxes.id is not None:
            if DEVICE == 'cuda':
                boxes = results[0].boxes.cpu()  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ CPU –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            else:
                boxes = results[0].boxes
        boxes_time = time.time() - boxes_start
        
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
        scaling_start = time.time()
        scaled_boxes = []
        if len(boxes) > 0:
            for box in boxes:
                if int(box.cls) == 0:  # –ö–ª–∞—Å—Å 0 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏—Ü—É
                    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    track_id = int(box.id.item())
                    
                    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    x1 = int(x1 * scale_x)
                    y1 = int(y1 * scale_y)
                    x2 = int(x2 * scale_x)
                    y2 = int(y2 * scale_y)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—Ç—É–ø—ã
                    padding = 20
                    x1 = max(0, x1 - padding)
                    y1 = max(0, y1 - padding)
                    x2 = min(frame.shape[1], x2 + padding)
                    y2 = min(frame.shape[0], y2 + padding)
                    
                    scaled_boxes.append({
                        'track_id': track_id,
                        'coords': (x1, y1, x2, y2),
                        'face_image': frame[y1:y2, x1:x2]
                    })
        scaling_time = time.time() - scaling_start
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –±–æ–∫—Å—ã
        processing_start = time.time()
        face_recognition_total = 0
        face_embedding_total = 0
        face_matching_total = 0
        drawing_total = 0
        
        for box_data in scaled_boxes:
            faces_processed += 1
            track_id = box_data['track_id']
            x1, y1, x2, y2 = box_data['coords']
            face_image = box_data['face_image']
            current_tracks.add(track_id)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–æ–≤—ã–π –ª–∏ —ç—Ç–æ —Ç—Ä–µ–∫
            if track_id not in tracked_faces:
                # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –ª–∏—Ü–æ
                face_start = time.time()
                embedding = get_face_embedding(face_image)
                face_embedding_total += time.time() - face_start
                
                if embedding is not None:
                    match_start = time.time()
                    match = find_matching_face(embedding)
                    face_matching_total += time.time() - match_start
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ tracked_faces —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
                    tracked_faces[track_id] = {
                        "name": match,
                        "first_seen": time.time(),
                        "last_seen": time.time()
                    }
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏ —Ä–∏—Å—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å
                    if match:
                        log_face_event(track_id, "recognized", match)
                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–∏—Ü–µ
                        cv2.putText(annotated_frame, match, (x1 + 100, y1 - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                        cv2.putText(annotated_frame, f"ID: {track_id}", (x1, y1 - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
                else:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥, —Å–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –±–µ–∑ –∏–º–µ–Ω–∏
                    tracked_faces[track_id] = {
                        "name": None,
                        "first_seen": time.time(),
                        "last_seen": time.time()
                    }
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è
            tracked_faces[track_id]["last_seen"] = time.time()
            
            # –ï—Å–ª–∏ –ª–∏—Ü–æ –µ—â–µ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ, –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å
            if tracked_faces[track_id]["name"] is None:
                face_start = time.time()
                embedding = get_face_embedding(face_image)
                face_embedding_total += time.time() - face_start
                
                if embedding is not None:
                    match_start = time.time()
                    match = find_matching_face(embedding)
                    if match:
                        # –ü—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∏–º—è –∏ –ª–æ–≥–∏—Ä—É–µ–º
                        tracked_faces[track_id]["name"] = match
                        log_face_event(track_id, "recognized", match)
                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–∏—Ü–µ
                        cv2.putText(annotated_frame, match, (x1 + 100, y1 - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                        cv2.putText(annotated_frame, f"ID: {track_id}", (x1, y1 - 10),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
                    face_matching_total += time.time() - match_start
                face_recognition_total += time.time() - face_start
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥ –≤ –∫–∞–¥—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç—Ä–µ–∫ –Ω–µ –±—ã–ª –∞–∫—Ç–∏–≤–Ω—ã–º
            if track_id not in active_tracks:
                if tracked_faces[track_id]["name"]:
                    log_face_event(track_id, "enter", tracked_faces[track_id]["name"])
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–∏—Ü–µ, –µ—Å–ª–∏ –æ–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ
            if tracked_faces[track_id]["name"]:
                name = tracked_faces[track_id]["name"]
                cv2.putText(annotated_frame, name, (x1 + 100, y1 - 10),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                cv2.putText(annotated_frame, f"ID: {track_id}", (x1, y1 - 10),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
        
        processing_time = time.time() - processing_start
        process_results_time = time.time() - process_results_start
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ —Ç—Ä–µ–∫–∏ –∏—Å—á–µ–∑–ª–∏
        check_tracks_start = time.time()
        for track_id in active_tracks - current_tracks:
            if track_id in tracked_faces:
                log_face_event(track_id, "exit", tracked_faces[track_id]["name"])
        check_tracks_time = time.time() - check_tracks_start
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
        active_tracks = current_tracks
        
        total_time = time.time() - start_time
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π
        logger.info(
            f"Frame {frame_id} processed in {process_results_time:.3f}s | "
            f"Copy: {copy_time:.3f}s | "
            f"Resize: {resize_time:.3f}s | "
            f"YOLO: {yolo_time:.3f}s | "
            f"Process results: {process_results_time:.3f}s ("
            f"boxes: {boxes_time:.3f}s, "
            f"scaling: {scaling_time:.3f}s, "
            f"processing: {processing_time:.3f}s ["
            f"embedding: {face_embedding_total:.3f}s, "
            f"matching: {face_matching_total:.3f}s, "
            f"drawing: {drawing_total:.3f}s]) | "
            f"Faces detected: {faces_processed} | "
            f"Active tracks: {len(active_tracks)}"
        )
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä
        out.write(annotated_frame)
        return True
    except Exception as e:
        logger.error(f"Error in process_frame: {e}")
        return False

def process_frames(out):
    frame_count = 0
    total_processing_time = 0
    
    while True:
        if not frame_queue.empty():
            frame = frame_queue.get()
            if frame is None:
                if frame_count > 0:
                    avg_time = total_processing_time / frame_count
                    logger.info(f"Average frame processing time: {avg_time:.3f}s over {frame_count} frames")
                break
            
            start_time = time.time()
            success = process_frame(frame, frame_count, out)
            processing_time = time.time() - start_time
            
            if success:
                total_processing_time += processing_time
                frame_count += 1

def open_capture_with_retry(url, max_retries=5, retry_delay=3):
    for attempt in range(max_retries):
        cap = cv2.VideoCapture(url)
        if cap.isOpened():
            print(f"‚úÖ Connected to stream on attempt {attempt+1}")
            return cap
        print(f"‚ö†Ô∏è Connection attempt {attempt+1}/{max_retries} failed, retrying in {retry_delay}s...")
        time.sleep(retry_delay)
    return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞—Ö–≤–∞—Ç–∞ –≤–∏–¥–µ–æ
cap = open_capture_with_retry(RTSP_INPUT_URL)
if cap is None:
    print("‚ùå Error: Could not connect to RTSP stream after multiple attempts!")
    exit()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∏–¥–µ–æ
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS) or 30

print(f"üé• Input stream opened: {frame_width}x{frame_height} at {fps} FPS")
print(f"üîÑ Forwarding to {RTSP_OUTPUT_URL}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GStreamer –¥–ª—è –≤—ã–≤–æ–¥–∞ RTSP —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
out = cv2.VideoWriter(
    f'appsrc ! videoconvert ! video/x-raw,format=I420 ! '
    f'x264enc speed-preset=ultrafast bitrate=1024 key-int-max={int(fps*2)} ! '
    f'video/x-h264,profile=baseline ! rtspclientsink protocols=tcp location={RTSP_OUTPUT_URL}',
    cv2.CAP_GSTREAMER, 0, fps, (frame_width, frame_height), True
)

if not out.isOpened():
    print("‚ùå Error: Cannot open RTSP output stream!")
    cap.release()
    exit()

# –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
processor_thread = Thread(target=process_frames, args=(out,))
processor_thread.start()

# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
frame_count = 0
start_time = time.time()

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("‚ùå Error: Failed to read frame from RTSP stream!")
        break

    frame_queue.put(frame)
    frame_count += 1

    # –í—ã–≤–æ–¥ FPS –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
    if frame_count % 900 == 0:  # 30 —Å–µ–∫—É–Ω–¥ –ø—Ä–∏ 30 FPS
        elapsed_time = time.time() - start_time
        current_fps = frame_count / elapsed_time
        print(f"Current FPS: {current_fps:.2f}")
        frame_count = 0
        start_time = time.time()

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞
frame_queue.put(None)
processor_thread.join()

# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
out.release()
cap.release()