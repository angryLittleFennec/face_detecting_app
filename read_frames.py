import cv2
import numpy as np
import dlib
import requests
from ultralytics import YOLO
from threading import Thread
from queue import Queue
import argparse
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ RTSP
RTSP_INPUT_URL = "rtsp://mediamtx:8554/mediamtx/stream"
RTSP_OUTPUT_URL = "rtsp://mediamtx:8554/5"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
FACE_MODEL = YOLO("ml_models/yolov8n-face.pt")  # –ú–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü
OBJECT_MODEL = YOLO("ml_models/yolov8n.pt")  # –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
HELMET_MODEL = YOLO("ml_models/hemletYoloV8_100epochs.pt")  # –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —à–ª–µ–º–æ–≤
face_detector = dlib.get_frontal_face_detector()
shape_predictor = dlib.shape_predictor("ml_models/shape_predictor_68_face_landmarks.dat")
face_rec_model = dlib.face_recognition_model_v1("ml_models/dlib_face_recognition_resnet_model_v1.dat")

used_faces = []  # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–∂–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ª–∏—Ü
url = "http://face_recognition:8000/find_face"
LOGGING_SERVICE_URL = "http://logging_service:8000/log"  # URL —Å–µ—Ä–≤–∏—Å–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è

# –û—á–µ—Ä–µ–¥—å –¥–ª—è –∫–∞–¥—Ä–æ–≤
frame_queue = Queue(maxsize=10)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ª–∏—Ü–∞
def get_face_embedding(image: np.ndarray):
    if image.size == 0:
        return None

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ RGB
    if image.shape[2] == 3 and image.dtype == np.uint8:
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    else:
        rgb_image = image

    # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü
    dets = face_detector(rgb_image, 1)
    if len(dets) == 0:
        return None

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –ª–∏—Ü–∞
    shape = shape_predictor(rgb_image, dets[0])

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ª–∏—Ü–∞
    face_descriptor = face_rec_model.compute_face_descriptor(rgb_image, shape, num_jitters=1)
    return np.array(face_descriptor)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ª–∏—Ü–∞
def find_matching_face(embedding):
    encoding_str = ",".join(map(str, embedding))
    params = {"embedding_str": encoding_str}

    response = requests.get(url, params=params)
    if response.status_code == 200:
        js = response.json()
        if js['status'] == 'success':
            return js['person']
        else:
            return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ª–æ–≥–æ–≤ –≤ —Å–µ—Ä–≤–∏—Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def send_log_to_service(log_message):
    try:
        response = requests.post(LOGGING_SERVICE_URL, json={"message": log_message})
        if response.status_code != 200:
            print(f"Failed to send log: {response.text}")
    except Exception as e:
        print(f"Error sending log: {e}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–æ–≤
def process_frames(parameters):
    while True:
        if not frame_queue.empty():
            frame = frame_queue.get()
            if frame is None:
                break

            annotated_frame = frame.copy()

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if "person" in parameters:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ü
                results = FACE_MODEL(frame)
                annotated_frame = results[0].plot(conf=False)

                # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü —Å –ø–æ–º–æ—â—å—é dlib
                for result in results:
                    for box in result.boxes:
                        if int(box.cls) == 0:  # –ö–ª–∞—Å—Å 0 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏—Ü—É –≤ YOLO
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            face_image = frame[y1:y2, x1:x2]

                            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –ª–∏—Ü–∞
                            embedding = get_face_embedding(face_image)
                            if embedding is not None:
                                # –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
                                match = None
                                for name, encoding in used_faces:
                                    dist = np.linalg.norm(encoding - embedding)
                                    min_dist = float("inf")
                                    if dist < 0.6 and dist < min_dist:
                                        match = name
                                        min_dist = dist
                                if match is None:
                                    match = find_matching_face(embedding)
                                    if match:
                                        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ª–∏—Ü–∞
                                        log_message = f"Person '{match}' entered the camera vision."
                                        send_log_to_service(log_message)
                                        used_faces.append((match, embedding))
                                if match:
                                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–º–µ–Ω–µ–º –Ω–∞ –∫–∞–¥—Ä
                                    cv2.putText(frame, match, (x1, y1 - 10),
                                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                                    cv2.rectangle(frame, (x1, y1), (x2, y2), color=(0, 255, 0))
                                    
            if "car" in parameters or "cell phone" in parameters or "traffic light" in parameters:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤
                results = OBJECT_MODEL(frame)
                annotated_frame = results[0].plot(conf=False)

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                for result in results:
                    for box in result.boxes:
                        class_id = int(box.cls)
                        class_name = OBJECT_MODEL.names[class_id]
                        if class_name in parameters:
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–º–µ–Ω–µ–º –∫–ª–∞—Å—Å–∞ –Ω–∞ –∫–∞–¥—Ä
                            cv2.putText(annotated_frame, class_name, (x1, y1 - 10),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

            if "helmet" in parameters:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —à–ª–µ–º–æ–≤
                results = HELMET_MODEL(frame)
                annotated_frame = results[0].plot(conf=False)

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                for result in results:
                    for box in result.boxes:
                        class_id = int(box.cls)
                        class_name = HELMET_MODEL.names[class_id]
                        if class_name == "helmet":  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª–∞—Å—Å "—à–ª–µ–º"
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–º–µ–Ω–µ–º –∫–ª–∞—Å—Å–∞ –Ω–∞ –∫–∞–¥—Ä
                            cv2.putText(annotated_frame, class_name, (x1, y1 - 10),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

            # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ –≤ –≤—ã—Ö–æ–¥–Ω–æ–π RTSP-–ø–æ—Ç–æ–∫
            out.write(frame)

# –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
parser = argparse.ArgumentParser(description="Process video stream with specific parameters.")
parser.add_argument(
    "--parameters", 
    nargs="+", 
    choices=["person", "car", "cell phone", "traffic light", "helmet"], 
    default=[],  # Changed default to an empty list
    help="List of parameters to process: person, car, cell phone, traffic light, helmet."
)
args = parser.parse_args()

# –û—Ç–∫—Ä—ã—Ç–∏–µ RTSP-–ø–æ—Ç–æ–∫–∞
cap = cv2.VideoCapture(RTSP_INPUT_URL)
if not cap.isOpened():
    print("‚ùå Error: Cannot open RTSP input stream!")
    exit()

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∏–¥–µ–æ
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

if fps <= 0:
    print("‚ö†Ô∏è Warning: FPS retrieval failed, setting default FPS to 30")
    fps = 30

print(f"üé• Input stream opened: {frame_width}x{frame_height} at {fps} FPS")
print(f"üîÑ Forwarding to {RTSP_OUTPUT_URL}")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GStreamer –¥–ª—è –≤—ã–≤–æ–¥–∞ RTSP
out = cv2.VideoWriter(
    f'appsrc ! videoconvert ! video/x-raw,format=I420 ! '
    f'x264enc speed-preset=ultrafast bitrate=1024 key-int-max={int(fps*2)} ! '
    f'video/x-h264,profile=baseline ! rtspclientsink protocols=tcp location={RTSP_OUTPUT_URL}',
    cv2.CAP_GSTREAMER, 0, fps, (frame_width, frame_height), True
)

if not out.isOpened():
    print("‚ùå Error: Cannot open RTSP output stream!")
    cap.release()
    exit()

# –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–æ–≤
processor_thread = Thread(target=process_frames, args=(args.parameters,))
processor_thread.start()

frame_ind = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("‚ùå Error: Failed to read frame from RTSP stream!")
        break

    frame_queue.put(frame)
    frame_ind += 1

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
frame_queue.put(None)
processor_thread.join()

# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
out.release()
cap.release()